## ğŸ§  GPT-from-Scratch

A minimal yet complete implementation of a GPT model in PyTorch, trained on WikiText-2 with a custom tokenizer.

---

### ğŸ“ Repository Structure

```
GPT-from-Scratch/
â”œâ”€â”€ config.py                # Configs: model, training, tokenizer, etc.
â”œâ”€â”€ dataset.py              # Dataset wrapper for WikiText + tokenizer handling
â”œâ”€â”€ model.py                # Transformer-based GPT architecture
â”œâ”€â”€ train.py                # Training script
â”œâ”€â”€ generate.py             # Text generation script (inference)
â”œâ”€â”€ utils.py                # Utility functions (optional)
â”œâ”€â”€ tokenizer_wikitext.json # Saved tokenizer (auto-generated)
â”œâ”€â”€ weights/                # Model checkpoints saved here
â”œâ”€â”€ run/                    # TensorBoard logs
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project overview & usage
```

---

### ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/GPT-from-Scratch.git
cd GPT-from-Scratch
pip install -r requirements.txt
```

---

### ğŸ§¾ Requirements

```txt
torch
datasets
tokenizers
tqdm
tensorboard
```

---

### ğŸš€ Usage

#### 1. **Training the model**

```bash
python train.py
```

Trains a GPT model on the WikiText-2 dataset. Tokenizer is built automatically if it doesn't exist.

#### 2. **Text generation (inference)**

```bash
python generate.py
```

Youâ€™ll be prompted to enter a custom prompt, and the model will generate a continuation using the trained checkpoint.

---

### ğŸ“Š Monitoring

Track training with TensorBoard:

```bash
tensorboard --logdir=run/
```

---

### ğŸ“š Example Output

```
Enter your prompt: The future of artificial intelligence
Generated text:
The future of artificial intelligence is uncertain, but it is likely to continue advancing in both complexity and utility...
```

---

### ğŸ›  Future Improvements (Suggestions)

* Add attention visualization tools
* Extend tokenizer support (BPE, SentencePiece)
* Train on larger datasets (e.g., OpenWebText)
* Add support for mixed precision training (AMP)
* Implement beam search sampling

---

### ğŸ“œ License

MIT License

---

Would you like me to generate the actual files in the right structure and upload them for easy copy-paste or GitHub setup?
